{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas as pd # Pandas is a library used for data manipulation and analysis, including reading and writing CSV files.\n",
    "from nltk.stem import PorterStemmer # Stemming algorithm used to reduce words to their base or root form.\n",
    "import re  # A module used for regular expression operations in Python.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # A vectorizer used to transform text data into numerical feature vectors\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd5746",
   "metadata": {},
   "source": [
    "## Predict my self-made dataset\n",
    "Here we are trying to use the trained svc model, trained with the data from the Sentiment/training.1600000.processed.noemoticon.csv to predict the labels of my self-made dataset. The main purpose here is to fill all the 30000 rows of my dataset in a faster way than manual labeling altough I will have to review the results later and check if the prediction is fine or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73645bc",
   "metadata": {},
   "source": [
    "This first part will be just formating and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ee84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe read is the same as CleanTranslateDataset/final_dataset_labeled.csv but I changed the name to be more accurate\n",
    "df_toPredict = pd.read_csv('./final_dataset_partially_labeled.csv',delimiter=',', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict.columns=['player', 'text', 'media', 'sentiment']\n",
    "\n",
    "df_toPredict = df_toPredict.fillna(999)\n",
    "df_toPredict = df_toPredict.replace([np.inf, -np.inf], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict.loc[df_toPredict['sentiment'] == 1, 'sentiment'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict['sentiment'] = df_toPredict['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82022d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_toPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a37e69",
   "metadata": {},
   "source": [
    "Now, as I am the one that created and labeled partially this dataset I know that I just labeled until the row 1420. After that row, there are the rows that we want to label using the svc model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2aace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict = df_toPredict.iloc[1420:]\n",
    "df_toPredict = df_toPredict.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b285cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a0bd8",
   "metadata": {},
   "source": [
    "Now I will just iterate over the dataset and use the functions defined above in order to label all the rows with a 1 or a 0. I won't label them with 0 or 4 because this task is done for being able to use the pre-trained BERT model in another notebook (BERT-fine-tuning-and-evaluation.ipynb), and this model needs 0's and 1's to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a85a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary functions to predict sentiment\n",
    "tfidf = TfidfVectorizer()\n",
    "with open('svc.pickle', 'rb') as handle:\n",
    "    svc = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b139a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "\"\"\"\n",
    "This function clean the passed text removing symbols and stemming it\n",
    "\"\"\"\n",
    "def clean_text(text):\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower().split()\n",
    "    lem = WordNetLemmatizer()\n",
    "    #text = [lem.lemmatize(word) for word in text if not word in stuff_to_be_removed]\n",
    "    text = [stemmer.stem(word) for word in text if word not in set(stuff_to_be_removed)]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "clean_text(\"I know you are really efficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72525d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function predicts the sentiment in a text\n",
    "\"\"\"\n",
    "def predict_sentiment_test(model, vectorizer, text):\n",
    "    # Clean the text removing stopwords and using a lemmatizer\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Vectorize the text using the TfidfVectorizer\n",
    "    text_vector = vectorizer.transform([text])\n",
    "    \n",
    "    # Make a prediction using the model\n",
    "    prediction = model.predict(text_vector)\n",
    "    \n",
    "    # Return the predicted sentiment\n",
    "    return prediction[0]\n",
    "\n",
    "\"\"\"\n",
    "This function predicts a dataset using the model and vectorizer passed by parameters and creates\n",
    "a new dataframe with the sentiment row filled with the predicted sentiment\n",
    "\"\"\"\n",
    "def predict_dataset(model, vectorizer, df):\n",
    "    # Create a dataframe that will contain the predictions\n",
    "    pred_df = df.copy()\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            # Call to the function that predicts text\n",
    "            predicted_sentiment = predict_sentiment_test(model, vectorizer, row['text'])\n",
    "\n",
    "            # Set the prediction to our valid values\n",
    "            if predicted_sentiment == 1:\n",
    "                predicted_sentiment = 1\n",
    "            else:\n",
    "                predicted_sentiment = 0\n",
    "\n",
    "            pred_df.at[index, 'sentiment'] = predicted_sentiment\n",
    "        except:\n",
    "            pred_df.at[index, 'sentiment'] = \"ERROR\"\n",
    "            continue\n",
    "\n",
    "        #print(f\"The sentiment of the sentence '{text}' is {predicted_sentiment}\")\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_dataset = predict_dataset(svc, tfidf, df_toPredict)\n",
    "predicted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc181b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the possible ERROR rows of the dataset labeled with the trained model\n",
    "predicted_dataset = df[~(df == 'ERROR').any(axis=1)]\n",
    "predicted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f02090",
   "metadata": {},
   "source": [
    "Save the progress as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('myDataset_Predicted.pickle', 'wb') as handle:\n",
    "#    pickle.dump(predicted_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7165b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_dataset.to_csv(\"PredictCleanDataset/myDataset_Predicted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
