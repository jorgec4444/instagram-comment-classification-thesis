{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f95a7a",
   "metadata": {
    "papermill": {
     "duration": 0.008454,
     "end_time": "2023-02-06T12:14:38.848560",
     "exception": false,
     "start_time": "2023-02-06T12:14:38.840106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ea25c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:38.867667Z",
     "iopub.status.busy": "2023-02-06T12:14:38.866997Z",
     "iopub.status.idle": "2023-02-06T12:14:40.695035Z",
     "shell.execute_reply": "2023-02-06T12:14:40.693950Z"
    },
    "papermill": {
     "duration": 1.839706,
     "end_time": "2023-02-06T12:14:40.697584",
     "exception": false,
     "start_time": "2023-02-06T12:14:38.857878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt  # Matplotlib is a data visualization library used for creating static, animated, and interactive visualizations in Python.\n",
    "import seaborn as sns  # Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "# Text Processing\n",
    "from string import punctuation  # A string of punctuation characters used for tokenizing and preprocessing text data.\n",
    "from nltk.tokenize import word_tokenize  # A tokenizer that splits text into words and punctuation marks, removing whitespace and other formatting characters.\n",
    "from nltk.corpus import stopwords  # A collection of common words that are often removed from text data before analysis, such as \"the,\" \"and,\" and \"a.\"\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer  # Stemming algorithms used to reduce words to their base or root form.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # A lemmatization algorithm used to reduce words to their base or root form, similar to stemming.\n",
    "\n",
    "# Data Processing\n",
    "import re  # A module used for regular expression operations in Python.\n",
    "import warnings  # A module used for handling warnings in Python.\n",
    "import numpy as np  # NumPy is a library used for working with arrays and numerical operations in Python.\n",
    "import pandas as pd  # Pandas is a library used for data manipulation and analysis, including reading and writing CSV files.\n",
    "\n",
    "# Machine Learning\n",
    "import pickle  # A module used for object serialization and deserialization in Python.\n",
    "from sklearn.naive_bayes import MultinomialNB  # A Naive Bayes classifier used for text classification tasks.\n",
    "from sklearn.svm import LinearSVC  # A Support Vector Machine classifier used for text classification tasks.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, PrecisionRecallDisplay, roc_curve  # A collection of metrics used for evaluating machine learning models.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # A Linear Discriminant Analysis classifier used for text classification tasks.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # A vectorizer used to transform text data into numerical feature vectors.\n",
    "from sklearn.linear_model import LogisticRegression  # A Logistic Regression classifier used for text classification tasks.\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, GroupShuffleSplit, LeaveOneOut, learning_curve, cross_val_score, LearningCurveDisplay # A module used for splitting data into training and testing sets.\n",
    "from wordcloud import WordCloud  # A data visualization technique used to display text data in a visual format, where the size of each word represents its frequency.\n",
    "import scipy.stats as stats\n",
    "from tqdm.auto import tqdm\n",
    "from fitter import Fitter\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79779f",
   "metadata": {
    "papermill": {
     "duration": 0.007875,
     "end_time": "2023-02-06T12:14:40.713862",
     "exception": false,
     "start_time": "2023-02-06T12:14:40.705987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading csv file and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc3bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:40.731704Z",
     "iopub.status.busy": "2023-02-06T12:14:40.730861Z",
     "iopub.status.idle": "2023-02-06T12:14:45.028884Z",
     "shell.execute_reply": "2023-02-06T12:14:45.027814Z"
    },
    "papermill": {
     "duration": 4.309779,
     "end_time": "2023-02-06T12:14:45.031653",
     "exception": false,
     "start_time": "2023-02-06T12:14:40.721874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./definitive_dataset.csv',delimiter=',', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf81be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:45.051061Z",
     "iopub.status.busy": "2023-02-06T12:14:45.049489Z",
     "iopub.status.idle": "2023-02-06T12:14:45.070086Z",
     "shell.execute_reply": "2023-02-06T12:14:45.069157Z"
    },
    "papermill": {
     "duration": 0.032172,
     "end_time": "2023-02-06T12:14:45.072291",
     "exception": false,
     "start_time": "2023-02-06T12:14:45.040119",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df13107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1668986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa340c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:45.089940Z",
     "iopub.status.busy": "2023-02-06T12:14:45.089665Z",
     "iopub.status.idle": "2023-02-06T12:14:45.266747Z",
     "shell.execute_reply": "2023-02-06T12:14:45.265110Z"
    },
    "papermill": {
     "duration": 0.188658,
     "end_time": "2023-02-06T12:14:45.269237",
     "exception": false,
     "start_time": "2023-02-06T12:14:45.080579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the type of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dadcce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:45.288241Z",
     "iopub.status.busy": "2023-02-06T12:14:45.287914Z",
     "iopub.status.idle": "2023-02-06T12:14:45.448819Z",
     "shell.execute_reply": "2023-02-06T12:14:45.447659Z"
    },
    "papermill": {
     "duration": 0.172935,
     "end_time": "2023-02-06T12:14:45.451502",
     "exception": false,
     "start_time": "2023-02-06T12:14:45.278567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if there is missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05946b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:45.542290Z",
     "iopub.status.busy": "2023-02-06T12:14:45.541989Z",
     "iopub.status.idle": "2023-02-06T12:14:45.548775Z",
     "shell.execute_reply": "2023-02-06T12:14:45.547868Z"
    },
    "papermill": {
     "duration": 0.018152,
     "end_time": "2023-02-06T12:14:45.550772",
     "exception": false,
     "start_time": "2023-02-06T12:14:45.532620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the dataset shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443dc12",
   "metadata": {
    "papermill": {
     "duration": 0.009198,
     "end_time": "2023-02-06T12:14:50.440003",
     "exception": false,
     "start_time": "2023-02-06T12:14:50.430805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the media values in the column are the same\n",
    "df['media'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6921ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:50.461479Z",
     "iopub.status.busy": "2023-02-06T12:14:50.459944Z",
     "iopub.status.idle": "2023-02-06T12:14:50.517576Z",
     "shell.execute_reply": "2023-02-06T12:14:50.516265Z"
    },
    "papermill": {
     "duration": 0.070115,
     "end_time": "2023-02-06T12:14:50.519691",
     "exception": false,
     "start_time": "2023-02-06T12:14:50.449576",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the unused column\n",
    "df=df.drop(columns=['media'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05f390",
   "metadata": {
    "papermill": {
     "duration": 0.008343,
     "end_time": "2023-02-06T12:14:45.567588",
     "exception": false,
     "start_time": "2023-02-06T12:14:45.559245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analysis with graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036723ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column just to analyze the comments length\n",
    "df['length'] = df['text'].apply(lambda x: len(str(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23988ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].value_counts().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22005471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(kind = 'hist' , bins = 200) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036cf86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the length of the comments via each sentiment\n",
    "ax = df.hist(column = 'length', by = 'sentiment', bins = 50 , figsize = (8, 8))\n",
    "plt.suptitle('Length via each Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4dc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the mean of the positive comments and the negative ones\n",
    "negative_mean = df.loc[df['sentiment'] == 0, 'length'].mean()\n",
    "positive_mean = df.loc[df['sentiment'] == 1, 'length'].mean()\n",
    "\n",
    "# Obtain the mean of the lenght of the overall comments\n",
    "length_mean = np.mean(df['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f862c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tags for the bars\n",
    "labels = ['Sentiment 0 length', 'Sentiment 1 length']\n",
    "\n",
    "values = [positive_mean, negative_mean]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.axhline(length_mean, color='red', linestyle='--', label='Overall mean')\n",
    "\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Mean')\n",
    "plt.title('Comments length mean by sentiment')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e593a7",
   "metadata": {},
   "source": [
    "Negative comments mean is highly above the positive comments mean so the negative comments are larger respecting comment length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61279eea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show how many examples belong to each side \n",
    "counts = df['sentiment'].value_counts()\n",
    "print(counts)\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.xticks(counts.index, ['1', '0'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae474c",
   "metadata": {},
   "source": [
    "After watching this graph I could expect an unbalanced classes problem when trying to train models with this difference, but since the difference is 68% (1) to 32% (0) I won't consider it as a huge umbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e23d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:14:50.575313Z",
     "iopub.status.busy": "2023-02-06T12:14:50.574521Z",
     "iopub.status.idle": "2023-02-06T12:14:50.579613Z",
     "shell.execute_reply": "2023-02-06T12:14:50.578722Z"
    },
    "papermill": {
     "duration": 0.018279,
     "end_time": "2023-02-06T12:14:50.581888",
     "exception": false,
     "start_time": "2023-02-06T12:14:50.563609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store in a variable all the text of the dataframe\n",
    "texts = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d5a75",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5333d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:15:11.624784Z",
     "iopub.status.busy": "2023-02-06T12:15:11.624142Z",
     "iopub.status.idle": "2023-02-06T12:15:11.654487Z",
     "shell.execute_reply": "2023-02-06T12:15:11.653224Z"
    },
    "papermill": {
     "duration": 0.043759,
     "end_time": "2023-02-06T12:15:11.656589",
     "exception": false,
     "start_time": "2023-02-06T12:15:11.612830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_to_be_removed = list(stopwords.words('english'))+list(punctuation) # List of stopwords and punctuation signs to be removed or ignored\n",
    "lem = WordNetLemmatizer()\n",
    "corpus = df['text'].tolist() # Create the corpus containing the list of all the texts of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4695e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_be_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff4827",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7980650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
    "         'driving', 'died', 'tried', 'feet', 'meeting']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Sentence lemmatization examples\n",
    "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
    "\n",
    "# Converting String into tokens\n",
    "list2 = nltk.word_tokenize(string)\n",
    "print(\"Tokenized sentence: \",list2, \"\\n\")\n",
    "\n",
    "lemmatized_string = ' '.join([lem.lemmatize(words) for words in list2])\n",
    "\n",
    "print(\"Lemmatized string: \",lemmatized_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e6914",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcc8cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:15:11.678487Z",
     "iopub.status.busy": "2023-02-06T12:15:11.678128Z",
     "iopub.status.idle": "2023-02-06T12:16:55.272113Z",
     "shell.execute_reply": "2023-02-06T12:16:55.271118Z"
    },
    "papermill": {
     "duration": 103.607523,
     "end_time": "2023-02-06T12:16:55.274566",
     "exception": false,
     "start_time": "2023-02-06T12:15:11.667043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove digits\n",
    "final_corpus = []\n",
    "for i in df.index:\n",
    "    try:\n",
    "        text = re.sub(\"(\\\\d|\\\\W)+\",\" \",df['text'][i])\n",
    "        text = re.sub(r'[ÂÃ]', 'A', text)\n",
    "        text = re.sub(r\"[şŝšś]\", \"s\", text)\n",
    "        text = re.sub(r\"[ĤĦĥħ]\", \"H\", text)\n",
    "        text = re.sub(r\"[ĆĈĊČćĉċč]\", \"c\", text)\n",
    "        #text = [lem.lemmatize(word) for word in text if word not in set(stuff_to_be_removed)] # Apply lemmatizing and remove stopwords\n",
    "        if text not in set(stuff_to_be_removed):\n",
    "            text = ''.join(text)\n",
    "            final_corpus.append(text)\n",
    "        else:\n",
    "            df.drop(i, axis=0, inplace=True)\n",
    "    except:\n",
    "        df.drop(i, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4a157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:16:55.296656Z",
     "iopub.status.busy": "2023-02-06T12:16:55.296365Z",
     "iopub.status.idle": "2023-02-06T12:16:55.605487Z",
     "shell.execute_reply": "2023-02-06T12:16:55.604297Z"
    },
    "papermill": {
     "duration": 0.322935,
     "end_time": "2023-02-06T12:16:55.608067",
     "exception": false,
     "start_time": "2023-02-06T12:16:55.285132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the final_corpus obtained in the cell above into the dataframe data_cleaned\n",
    "data_cleaned = pd.DataFrame()\n",
    "data_cleaned[\"text\"] = final_corpus\n",
    "data_cleaned[\"sentiment\"] = df[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57915d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:16:55.631006Z",
     "iopub.status.busy": "2023-02-06T12:16:55.630704Z",
     "iopub.status.idle": "2023-02-06T12:16:56.298917Z",
     "shell.execute_reply": "2023-02-06T12:16:56.297762Z"
    },
    "papermill": {
     "duration": 0.682584,
     "end_time": "2023-02-06T12:16:56.301822",
     "exception": false,
     "start_time": "2023-02-06T12:16:55.619238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the final_corpus obtained in the cell above into the dataframe data_eda\n",
    "data_eda = pd.DataFrame()\n",
    "data_eda['text'] = final_corpus\n",
    "data_eda['sentiment'] = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb44181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:16:56.323774Z",
     "iopub.status.busy": "2023-02-06T12:16:56.323497Z",
     "iopub.status.idle": "2023-02-06T12:16:56.389063Z",
     "shell.execute_reply": "2023-02-06T12:16:56.388018Z"
    },
    "papermill": {
     "duration": 0.079132,
     "end_time": "2023-02-06T12:16:56.391443",
     "exception": false,
     "start_time": "2023-02-06T12:16:56.312311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the positive and negative labeled comments\n",
    "positive = data_eda[data_eda['sentiment'] == 1]\n",
    "positive_list = positive['text'].tolist()\n",
    "negative = data_eda[data_eda['sentiment'] == 0]\n",
    "negative_list = negative['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcd83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:16:56.413645Z",
     "iopub.status.busy": "2023-02-06T12:16:56.413351Z",
     "iopub.status.idle": "2023-02-06T12:16:57.187107Z",
     "shell.execute_reply": "2023-02-06T12:16:57.186107Z"
    },
    "papermill": {
     "duration": 0.787732,
     "end_time": "2023-02-06T12:16:57.189996",
     "exception": false,
     "start_time": "2023-02-06T12:16:56.402264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_all = \"\".join([word for sent in positive_list for word in sent ])\n",
    "negative_all = \"\".join([word for sent in negative_list for word in sent ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eff68",
   "metadata": {
    "papermill": {
     "duration": 0.010664,
     "end_time": "2023-02-06T12:16:57.211725",
     "exception": false,
     "start_time": "2023-02-06T12:16:57.201061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word cloud positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8b07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:16:57.234240Z",
     "iopub.status.busy": "2023-02-06T12:16:57.233877Z",
     "iopub.status.idle": "2023-02-06T12:17:13.307243Z",
     "shell.execute_reply": "2023-02-06T12:17:13.306271Z"
    },
    "papermill": {
     "duration": 16.093782,
     "end_time": "2023-02-06T12:17:13.316284",
     "exception": false,
     "start_time": "2023-02-06T12:16:57.222502",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate and display a word cloud for non-offensive words\n",
    "WordCloud()\n",
    "wordcloud = WordCloud(width=1000,\n",
    "                      height=500,\n",
    "                      background_color='skyblue',\n",
    "                      max_words = 90).generate(positive_all)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title(\"Non-offensive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13172b76",
   "metadata": {
    "papermill": {
     "duration": 0.017813,
     "end_time": "2023-02-06T12:17:13.352296",
     "exception": false,
     "start_time": "2023-02-06T12:17:13.334483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word cloud negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bebb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T12:17:13.392965Z",
     "iopub.status.busy": "2023-02-06T12:17:13.392581Z",
     "iopub.status.idle": "2023-02-06T12:18:00.369307Z",
     "shell.execute_reply": "2023-02-06T12:18:00.368422Z"
    },
    "papermill": {
     "duration": 47.034154,
     "end_time": "2023-02-06T12:18:00.406300",
     "exception": false,
     "start_time": "2023-02-06T12:17:13.372146",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "WordCloud()\n",
    "wordcloud = WordCloud(width=1000,\n",
    "                      height=500,\n",
    "                      background_color='skyblue',\n",
    "                      max_words = 90).generate(negative_all)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title(\"Offensive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587696af",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543d027",
   "metadata": {
    "papermill": {
     "duration": 0.025588,
     "end_time": "2023-02-06T12:18:00.457938",
     "exception": false,
     "start_time": "2023-02-06T12:18:00.432350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFIDF or CountVectorizer for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13535e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "tfidf = TfidfVectorizer(use_idf=True)\n",
    "bigram_tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True) # We use (1,2) because if we use only (2,2) we will only consider bigrams and not individual words\n",
    "\n",
    "# Check if he ngram creatiion of bigrams is correct\n",
    "analyze = bigram_tfidf.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the corpus\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "xt_bi = bigram_tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc852cb",
   "metadata": {},
   "source": [
    "### Check what the vectorizers return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the indexes of the numeric representation of each n_gram (word)\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, the index of the word goat in the vectorizer is the 803 \n",
    "print(tfidf.vocabulary_['goat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ade5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectorizer with unigrams features and the amount of features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"Amount of features: \",len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f619338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectorizer features of a word\n",
    "feature_names = tfidf.get_feature_names_out()[803]\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5da775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IDF values associated to each feature\n",
    "print(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectorizer with bigrams features and the amount of features\n",
    "feature_names = bigram_tfidf.get_feature_names_out()\n",
    "print(feature_names)\n",
    "print(\"Amount of features: \",len(bigram_tfidf.idf_)) # The amount of features is almost the triple than using only unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c701bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See the difference between this vocabulary and the other one with unigrams\n",
    "# Here we see two words toghether and also unigrams and we were not seeing that on the previous one\n",
    "print(bigram_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c5f30",
   "metadata": {},
   "source": [
    "## See how the amount of features affects vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e6514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Different features list\n",
    "num_features = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "results = {'Logistic Regression': [], 'Decision Tree': [], 'Linear SVC': [], 'Multinomial NB': [], 'Random Forest': []}\n",
    "\n",
    "for n in num_features:\n",
    "\n",
    "    bigram_tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_df=0.5, max_features=n)\n",
    "    xt_bi = bigram_tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    models = [\n",
    "        ('Logistic Regression', LogisticRegression()),\n",
    "        ('Decision Tree', DecisionTreeClassifier()),\n",
    "        ('Linear SVC', LinearSVC()),\n",
    "        ('Multinomial NB', MultinomialNB()),\n",
    "        ('Random Forest', RandomForestClassifier())\n",
    "    ]\n",
    "    \n",
    "    for model_name, model in models:\n",
    "        accuracies = []\n",
    "\n",
    "        for train_index, test_index in kfold.split(xt_bi, y):\n",
    "            X_train, X_test = xt_bi[train_index], xt_bi[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            accuracy = accuracy_score(y_train, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        average_accuracy = np.mean(accuracies)\n",
    "        results[model_name].append(average_accuracy)\n",
    "            \n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xticks(num_features)\n",
    "for model_name, accuracies in results.items():\n",
    "    plt.plot(num_features, accuracies, label=model_name)\n",
    "\n",
    "plt.xlabel('Número de Características')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Precisión en función del número de características')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = ['Logistic Regression', 'Decision Tree', 'Linear SVC', 'Multinomial NB', 'Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuress = {'0': 1000, '1': 2000, '2': 3000, '3': 4000, '4': 5000, '5': 6000, '6': 7000, '7': 8000, '8': 9000, '9': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68289c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['Model', 'Number of Features', 'Accuracy'])\n",
    "\n",
    "for model in models_names:\n",
    "    feature = results[model].index(max(results[model]))\n",
    "    accuracy = results[model][results[model].index(max(results[model]))]\n",
    "    feature_name = featuress[str(feature)]\n",
    "    results_df = results_df.append({'Model': model, 'Number of Features': feature_name, 'Accuracy': accuracy}, ignore_index=True)\n",
    "\n",
    "print(\"TF-IDF vectorizer results\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6ff58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Different features list\n",
    "num_features2 = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "# Dict to store the results\n",
    "results2 = {'Logistic Regression': [], 'Decision Tree': [], 'Linear SVC': [], 'Multinomial NB': [], 'Random Forest': []}\n",
    "\n",
    "for n in num_features2:\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), max_df=0.5, max_features=n)\n",
    "    xt_bi_countvec = vectorizer.fit_transform(data_cleaned[\"text\"])\n",
    "    \n",
    "    models = [\n",
    "        ('Logistic Regression', LogisticRegression()),\n",
    "        ('Decision Tree', DecisionTreeClassifier()),\n",
    "        ('Linear SVC', LinearSVC()),\n",
    "        ('Multinomial NB', MultinomialNB()),\n",
    "        ('Random Forest', RandomForestClassifier())\n",
    "    ]\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for model_name, model in models:\n",
    "        accuracies = []  # List to store the accuracies on each fold\n",
    "\n",
    "        for train_index, test_index in kfold.split(xt_bi_countvec, y):\n",
    "            X_train, X_test = xt_bi_countvec[train_index], xt_bi_countvec[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            accuracy = accuracy_score(y_train, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        average_accuracy = np.mean(accuracies)\n",
    "        results2[model_name].append(average_accuracy)\n",
    "            \n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xticks(num_features)\n",
    "for model_name, accuracies in results2.items():\n",
    "    plt.plot(num_features, accuracies, label=model_name)\n",
    "\n",
    "plt.xlabel('Número de Características')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Precisión en función del número de características')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2120e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.DataFrame(columns=['Model', 'Number of Features', 'Accuracy'])\n",
    "\n",
    "for model in models_names:\n",
    "    feature = results2[model].index(max(results2[model]))\n",
    "    accuracy = results2[model][results2[model].index(max(results2[model]))]\n",
    "    feature_name = featuress[str(feature)]\n",
    "    results_df2 = results_df2.append({'Model': model, 'Number of Features': feature_name, 'Accuracy': accuracy}, ignore_index=True)\n",
    "\n",
    "print(\"CountVectorizer results\")\n",
    "results_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4fd9d",
   "metadata": {},
   "source": [
    "### Check if TFIDF vectorizer is better than CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701ab53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "bigram_tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_df=0.5)\n",
    "xt_bi = bigram_tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt_bi, y):\n",
    "    X_train, X_test = xt_bi[train_index], xt_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model1.fit(X_train, y_train)\n",
    "    y_pred = model1.predict(X_train)\n",
    "    \n",
    "    all_predictions.extend(y_pred)\n",
    "    all_true_labels.extend(y_train)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(\"testing accuracy = \", accuracy*100)\n",
    "print(classification_report(all_true_labels, all_predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "# Get the total acmount of examples\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create the confusion matrix with the success and failure percentages\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d7525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# COUNT VECTORIZER\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_df=0.5, max_features=4000)\n",
    "xt_bi_countvec = vectorizer.fit_transform(data_cleaned[\"text\"])\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt_bi_countvec, y):\n",
    "    X_train, X_test = xt_bi_countvec[train_index], xt_bi_countvec[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model3.fit(X_train, y_train)\n",
    "    y_pred = model3.predict(X_train)\n",
    "    \n",
    "    all_predictions.extend(y_pred)\n",
    "    all_true_labels.extend(y_train)\n",
    "\n",
    "# Get the accuracy\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(all_true_labels, all_predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "# Get the total acmount of examples\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of example\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create the confusion matrix with the success and failure percentages\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion matrix (Percentage)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9dd2b",
   "metadata": {
    "papermill": {
     "duration": 0.025124,
     "end_time": "2023-02-06T12:18:11.573168",
     "exception": false,
     "start_time": "2023-02-06T12:18:11.548044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Test Split / Splitter classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cf336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "classifier = model5\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_test_pred = classifier.predict(X_train)\n",
    "    \n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "\n",
    "metrics(y_train,y_train_pred,y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fb0ca",
   "metadata": {},
   "source": [
    "### Metrics used to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Function that uses the predictions of the model to get a report of the results, includes the accuracy score, a ConfusionMatrix\n",
    "# and some other useful metrics\n",
    "def metrics(y_train,y_train_pred,y_test,y_test_pred):\n",
    "    print(\"training accuracy = \",round(accuracy_score(y_train,y_train_pred),2)*100)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_train,y_train_pred,normalize = 'all')\n",
    "    print(classification_report(y_train,y_train_pred))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"testing accuracy = \",round(accuracy_score(y_test,y_test_pred),2)*100)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test,y_test_pred,normalize = 'all')\n",
    "    print(classification_report(y_test,y_test_pred))\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025f4fc",
   "metadata": {},
   "source": [
    "### To plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181741d",
   "metadata": {
    "papermill": {
     "duration": 0.025692,
     "end_time": "2023-02-06T12:18:39.353650",
     "exception": false,
     "start_time": "2023-02-06T12:18:39.327958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can just change the model name and object to pickle depending on wich model do we want to dump\n",
    "# Saving the model to a pickle object in order to access to the data later\n",
    "#with open('dtTFG.pickle', 'wb') as handle:\n",
    "#    pickle.dump(dt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d94c99",
   "metadata": {},
   "source": [
    "## Cross val score of the default models with the default data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting analyzing we create the models and a list with them\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2c48a",
   "metadata": {},
   "source": [
    "We will see the accuracy of te default models with the default dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826620e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xt_bi = bigram_tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt_bi, y):\n",
    "    X_train, X_test = xt_bi[train_index], xt_bi[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Create a dict to store the models' scores\n",
    "first_results = {}\n",
    "\n",
    "scores = []\n",
    "for model in models:\n",
    "    # Evaluate the model with cross_validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Store the scores\n",
    "    scores.append(cv_scores.mean())\n",
    "\n",
    "# Add the scores to the dict\n",
    "first_results[\"Score\"] = scores\n",
    "\n",
    "# Create a dataframe from the dict\n",
    "first_results_df = pd.DataFrame(first_results)\n",
    "\n",
    "# Mapping dict with the indices related to the models names\n",
    "model_names = {0: 'LogisticRegression', 1: 'DecisionTree', 2: 'LinearSVC', 3: 'NB', 4: 'RandomForest'}\n",
    "\n",
    "# Change the indexes for the models names\n",
    "first_results_df.rename(index=model_names, inplace=True)\n",
    "\n",
    "# Store the dataframe in a CSV file\n",
    "first_results_df.to_csv('first_results_df_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108c2d8",
   "metadata": {},
   "source": [
    "# Precision-recall curve, ROC curve & Learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b4534",
   "metadata": {},
   "source": [
    "This metric will show us each model ability to distinguish between positive and negative classes across different probability thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d04ec",
   "metadata": {},
   "source": [
    "We will plot see and plot the default scores of each model with the optimal vectorizer configuration but the default hyperparameter configuration and finally we will see all toghether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d18347",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = count_vec.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee424cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433649d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linearSvc = LinearSVC()\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    linearSvc.fit(X_train, y_train)\n",
    "    y_pred = linearSvc.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"LinearSVC confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision1, recall1, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc1 = auc(recall1, precision1)\n",
    "\n",
    "print('LinearSVC AUC = %0.2f' % pr_auc1)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision1, recall=recall1)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0d20f",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "y_scores = linearSvc.decision_function(X_train)\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the curve ROC (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='LinearSVC (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Línea de referencia\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve  - LinearSVC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9eea5",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c454c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(linearSvc, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e06bd",
   "metadata": {},
   "source": [
    "## Trying to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47263452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters search space\n",
    "parameters = {\n",
    "    'C': [0.2, 0.3],\n",
    "    'dual': [True, False],\n",
    "    'fit_intercept': [True],\n",
    "    'multi_class': ['crammer_singer', 'ovr']\n",
    "}\n",
    "\n",
    "# Define the cross_validation with 5 fold\n",
    "cv = 5\n",
    "\n",
    "# Do the hyperparameters search with GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=LinearSVC(), param_grid=parameters, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37986b65",
   "metadata": {},
   "source": [
    "Finally the best params for this linearSVC model is only changing the value of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c41fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linearSvc = LinearSVC(random_state=42, C=0.2, fit_intercept=True, dual=False, multi_class='crammer_singer',\n",
    "                      class_weight={0:0.6, 1:0.4})\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    linearSvc.fit(X_train, y_train)\n",
    "    y_pred = linearSvc.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"LinearSVC confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision1, recall1, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc1 = auc(recall1, precision1)\n",
    "\n",
    "print('LinearSVC AUC = %0.2f' % pr_auc1)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision1, recall=recall1)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99627f",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "y_scores = linearSvc.decision_function(X_train)\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the curve ROC (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='LinearSVC (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Línea de referencia\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve  - LinearSVC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b707c",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534979c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(linearSvc, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b3390",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4281d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logisticRegression = LogisticRegression()\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=4000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    logisticRegression.fit(X_train, y_train)\n",
    "    y_pred = logisticRegression.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"LogisticRegression Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision2, recall2, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc2 = auc(recall2, precision2)\n",
    "\n",
    "print('Logistic Regression AUC = %0.2f' % pr_auc2)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision2, recall=recall2)\n",
    "                              \n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = logisticRegression.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d15b97",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a094cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(logisticRegression, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d8407",
   "metadata": {},
   "source": [
    "## Trying to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters search space\n",
    "parameters = {\n",
    "    'dual': [True],\n",
    "    'fit_intercept': [True],\n",
    "    'solver': ['liblinear'],\n",
    "    'random_state':[42],\n",
    "    'l1_ratio':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Define the cross_validation with 5 fold\n",
    "cv = 5\n",
    "\n",
    "# Do the hyperparameters search with GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=parameters, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392f748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logisticRegression = LogisticRegression(random_state=42, fit_intercept=True, solver='liblinear', dual=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=4000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    logisticRegression.fit(X_train, y_train)\n",
    "    y_pred = logisticRegression.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"LogisticRegression Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision2, recall2, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc2 = auc(recall2, precision2)\n",
    "\n",
    "print('Logistic Regression AUC = %0.2f' % pr_auc2)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision2, recall=recall2)\n",
    "                              \n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = logisticRegression.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fab05c",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fed2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(logisticRegression, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420eed6",
   "metadata": {},
   "source": [
    "## (parenthesis) Checking if threshold affects auc accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae42d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Obtener las probabilidades estimadas de pertenencia a cada clase\n",
    "proba = logisticRegression.predict_proba(X_test)\n",
    "\n",
    "# Definir umbral inicial\n",
    "threshold = 0.5\n",
    "\n",
    "# Clasificar las instancias basado en las probabilidades estimadas\n",
    "y_pred = (proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión y el recall para el umbral inicial\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Umbral: {threshold}\")\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Score:\", precision-recall)\n",
    "\n",
    "# Ajustar el umbral y volver a clasificar las instancias\n",
    "threshold = 0.6\n",
    "y_pred = (proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión y el recall para el nuevo umbral\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nUmbral: {threshold}\")\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Score:\", precision-recall)\n",
    "\n",
    "# Ajustar el umbral y volver a clasificar las instancias\n",
    "threshold = 0.7\n",
    "y_pred = (proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión y el recall para el nuevo umbral\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nUmbral: {threshold}\")\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Score:\", precision-recall)\n",
    "\n",
    "# Ajustar el umbral y volver a clasificar las instancias\n",
    "threshold = 0.8\n",
    "y_pred = (proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión y el recall para el nuevo umbral\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nUmbral: {threshold}\")\n",
    "print(f\"Precisión: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(\"Score:\", precision-recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa171a96",
   "metadata": {},
   "source": [
    "After this check, we can say that the threshold can affect to classification criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496fadd",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e923ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multinomialNB = MultinomialNB()\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=5000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    multinomialNB.fit(X_train, y_train)\n",
    "    y_pred = multinomialNB.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"MultinomialNB Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision3, recall3, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc3 = auc(recall3, precision3)\n",
    "\n",
    "print('MultinomialNB AUC = %0.2f' % pr_auc3)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision3, recall=recall3)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c457a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = multinomialNB.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='MultinomialNB (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - MultinomialNB')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bb1ae",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(multinomialNB, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceae50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd252b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8713d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB.feature_log_prob_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6e959",
   "metadata": {},
   "source": [
    "## Trying to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters search space\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1, 1.5, 2],\n",
    "    'fit_prior': [True, False],\n",
    "    'class_prior': [None, [0.2, 0.8], [0.5, 0.5], [0.8, 0.2], [0.3, 0.7]]\n",
    "}\n",
    "\n",
    "# Define the cross_validation with 5 fold\n",
    "cv = 5\n",
    "\n",
    "# Do the hyperparameters search with GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=MultinomialNB(), param_grid=parameters, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3519f",
   "metadata": {},
   "source": [
    "After trying to fit the model with different parameters combinations the model does not improve so it will remain as it was by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb442da",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd67dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decisionTreeClassifier = DecisionTreeClassifier()\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    decisionTreeClassifier.fit(X_train, y_train)\n",
    "    y_pred = decisionTreeClassifier.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"DecisionTreeClassifier Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision4, recall4, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc4 = auc(recall4, precision4)\n",
    "\n",
    "print('DecisionTreeClassifier AUC = %0.2f' % pr_auc4)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision4, recall=recall4)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109accdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = decisionTreeClassifier.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='DecisionTreeClassifier (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - DecisionTreeClassifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4db6b5",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162f4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(decisionTreeClassifier, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5d3c3",
   "metadata": {},
   "source": [
    "## Trying to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters search space\n",
    "parameters = {\n",
    "    'max_depth': [6],\n",
    "    'min_samples_leaf': [2,3,4,5,6,7,8,9,10],\n",
    "    'max_leaf_nodes': [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Define the cross_validation with 5 fold\n",
    "cv = 5\n",
    "\n",
    "# Do the hyperparameters search with GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=parameters, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fc5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decisionTreeClassifier = DecisionTreeClassifier(random_state=42, max_depth=6, min_samples_leaf=4, max_leaf_nodes=14)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    decisionTreeClassifier.fit(X_train, y_train)\n",
    "    y_pred = decisionTreeClassifier.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"DecisionTreeClassifier Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision4, recall4, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc4 = auc(recall4, precision4)\n",
    "\n",
    "print('DecisionTreeClassifier AUC = %0.2f' % pr_auc4)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision4, recall=recall4)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843621fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = decisionTreeClassifier.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='DecisionTreeClassifier (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - DecisionTreeClassifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fd6c1",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb76ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(decisionTreeClassifier, xt, y, score_type=\"both\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fcbec",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc462f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randomForestClassifier = RandomForestClassifier()\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    randomForestClassifier.fit(X_train, y_train)\n",
    "    y_pred = randomForestClassifier.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"RandomForestClassifier Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision5, recall5, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc5 = auc(recall5, precision5)\n",
    "\n",
    "print('RandomForestClassifier AUC = %0.2f' % pr_auc5)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision5, recall=recall5)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c81f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = randomForestClassifier.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='RandomForestClassifier (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - RandomForestClassifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafa75c",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80511d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(randomForestClassifier, xt, y, score_type=\"both\", n_jobs=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a148",
   "metadata": {},
   "source": [
    "## Trying to improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1294330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters search space\n",
    "parameters = {\n",
    "    'max_depth': [10,12,15,16,18,20],\n",
    "    'n_estimators': [50,100,120,150],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'max_leaf_nodes': [10,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Define the cross_validation with 5 fold\n",
    "cv = 5\n",
    "\n",
    "# Do the hyperparameters search with GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model with the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best accuracy score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd245af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randomForestClassifier = RandomForestClassifier(random_state=42, n_jobs=3, max_depth=10, n_estimators=120, min_samples_leaf=2,\n",
    "                                               class_weight={0:0.56, 1:0.44}, max_leaf_nodes=12)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), use_idf=True, max_features=1000)\n",
    "xt = tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "y = data_cleaned['sentiment']\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the generated folds by StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    randomForestClassifier.fit(X_train, y_train)\n",
    "    y_pred = randomForestClassifier.predict(X_train)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Training accuracy = \", accuracy*100)\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "# Get the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Get the total examples amount\n",
    "total_examples = np.sum(confusion)\n",
    "\n",
    "# Calculate the success rate by the total amount of examples\n",
    "confusion_percentage = confusion / total_examples\n",
    "\n",
    "# Create a graphic of the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_percentage, annot=True, cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"RandomForestClassifier Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision5, recall5, _ = precision_recall_curve(true_labels, predictions)\n",
    "pr_auc5 = auc(recall5, precision5)\n",
    "\n",
    "print('RandomForestClassifier AUC = %0.2f' % pr_auc5)\n",
    "\n",
    "disp = PrecisionRecallDisplay(precision=precision5, recall=recall5)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30015c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities prediction of the model\n",
    "proba = randomForestClassifier.predict_proba(X_train)\n",
    "\n",
    "# Get the probabilities for the positive class (class index 1)\n",
    "y_scores = proba[:, 1]\n",
    "\n",
    "# Get the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# Get the area under the ROC curve (AUC-ROC)\n",
    "auc_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label='RandomForestClassifier (AUC = %0.2f)' % auc_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Reference line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - RandomForestClassifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa3498",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c5abb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LearningCurveDisplay.from_estimator(randomForestClassifier, xt, y, score_type=\"both\", n_jobs=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10320e1",
   "metadata": {},
   "source": [
    "# Comparing models learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14538334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model 1: LogisticRegression()\n",
    "Model 2: DecisionTreeClassifier()\n",
    "Model 3: LinearSVC()\n",
    "Model 4: MultinomialNB()\n",
    "Model 5: RandomForestClassifier()\n",
    "\"\"\"\n",
    "\n",
    "# Define the training sizes for the learning curves\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "models = [linearSvc, logisticRegression, multinomialNB, decisionTreeClassifier, randomForestClassifier]\n",
    "model_names = ['Linear SVC', 'Logistic Regression', 'Multinomial Naive Bayes', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    train_sizes_abs, train_scores, test_scores = learning_curve(model, X_train, y_train, train_sizes=train_sizes)\n",
    "    \n",
    "    # Get the means and the std in the test and train set\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Get the learning curves and the legend\n",
    "    #plt.plot(train_sizes_abs, train_mean, 'o-', label='Model {}'.format(model_names[i] + ' (Train)')\n",
    "    #plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    \n",
    "    plt.plot(train_sizes_abs, test_mean, 'o-', label='{}'.format(model_names[i] + ' (Test)'))\n",
    "    plt.fill_between(train_sizes_abs, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('Learning curves for models over the test set')\n",
    "plt.xlabel('Test size')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95f091",
   "metadata": {},
   "source": [
    "*--------------------------------------------------------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38db8e",
   "metadata": {},
   "source": [
    "# Will not be considered from this cell onwards as it has not been included in the report and will be left for future work.\n",
    "\n",
    "The results and functions used here maybe are not update and correctly implemented as it has been just ane exploratory research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918c884",
   "metadata": {},
   "source": [
    "*--------------------------------------------------------------------------------------------------------------------------------*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52699fe0",
   "metadata": {},
   "source": [
    "## Analyzing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523d37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [linearSvc, logisticRegression, multinomialNB, decisionTreeClassifier, randomForestClassifier]\n",
    "scores_mean = []\n",
    "scores_std = []\n",
    "\n",
    "# Realizar la validación cruzada para cada modelo\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    scores_mean.append(scores.mean())\n",
    "    scores_std.append(scores.std())\n",
    "\n",
    "# Imprimir los puntajes obtenidos en cada modelo\n",
    "for i, model in enumerate(models):\n",
    "    print(\"Model\", i+1)\n",
    "    print(\"Cross validation scores:\", scores[i])\n",
    "    print(\"Scores mean:\", scores_mean[i])\n",
    "    print(\"Standard deviation of the score:\", scores_std[i])\n",
    "    print()\n",
    "\n",
    "# Generar gráfico comparativo de los modelos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(range(1, len(models)+1), scores_mean, yerr=scores_std, fmt='o-', capsize=5)\n",
    "plt.xticks(range(1, len(models)+1))\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Cross validation score comparative')\n",
    "plt.savefig('grafico_cross_validation_5_models.png', dpi=300, pad_inches=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a3070",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fc297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "\"\"\"\n",
    "Returns a new corpus after applying data augmentation\n",
    "\"\"\"\n",
    "def augment_with_synonyms(corpus):\n",
    "    augmented_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        augmented_tokens = []\n",
    "        for token in tokens:\n",
    "            synonyms = get_synonyms(token)\n",
    "            if synonyms:\n",
    "                augmented_tokens.append(synonyms[0])\n",
    "            else:\n",
    "                augmented_tokens.append(token)\n",
    "        augmented_sentence = ' '.join(augmented_tokens)\n",
    "        augmented_corpus.append(augmented_sentence)\n",
    "    return augmented_corpus\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns a new corpus after applying data augmentation\n",
    "\"\"\"\n",
    "def augment_with_shuffle_words(corpus):\n",
    "    augmented_corpus = []\n",
    "    for text in corpus:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        random.shuffle(tokens)\n",
    "        shuffled_text = ' '.join(tokens)\n",
    "        augmented_corpus.append(shuffled_text)\n",
    "    return augmented_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    # Remove digits\n",
    "    final_corpus = []\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            text = re.sub(\"(\\\\d|\\\\W)+\",\" \",df['text'][i])\n",
    "            text = re.sub(r'[ÂÃ]', 'A', text)\n",
    "            text = re.sub(r\"[şŝšś]\", \"s\", text)\n",
    "            text = re.sub(r\"[ĤĦĥħ]\", \"H\", text)\n",
    "            text = re.sub(r\"[ĆĈĊČćĉċč]\", \"c\", text)\n",
    "            #text = [lem.lemmatize(word) for word in text if word not in set(stuff_to_be_removed)] # Apply lemmatizing and remove stopwords\n",
    "            if text not in set(stuff_to_be_removed):\n",
    "                text = ''.join(text)\n",
    "                final_corpus.append(text)\n",
    "            else:\n",
    "                df.drop(i, axis=0, inplace=True)\n",
    "        except:\n",
    "            df.drop(i, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus_augmented = []\n",
    "for i in df.index:\n",
    "    try:\n",
    "        text = re.sub(\"(\\\\d|\\\\W)+\",\" \",df['text'][i])\n",
    "        text = re.sub(r'[ÂÃ]', 'A', text)\n",
    "        text = re.sub(r\"[şŝšś]\", \"s\", text)\n",
    "        text = re.sub(r\"[ĤĦĥħ]\", \"H\", text)\n",
    "        text = re.sub(r\"[ĆĈĊČćĉċč]\", \"c\", text)\n",
    "        #text = [lem.lemmatize(word) for word in text if word not in set(stuff_to_be_removed)] # Apply lemmatizing and remove stopwords\n",
    "        if text not in set(stuff_to_be_removed):\n",
    "            text = ''.join(text)\n",
    "            final_corpus_augmented.append(text)\n",
    "        else:\n",
    "            df.drop(i, axis=0, inplace=True)\n",
    "    except:\n",
    "        df.drop(i, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aabf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size is the same, it's just making better the data that was already there\n",
    "final_corpus_augmented = augment_with_synonyms(final_corpus_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743942c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd972225",
   "metadata": {},
   "source": [
    "### Sutdy the cross val score of the models using different data augmented sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66301c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_default(corpus, models):\n",
    "    # Crear un diccionario vacío para almacenar las puntuaciones de los modelos\n",
    "    results = {}\n",
    "    \n",
    "    # This part is only for adding the scores for the default dataset\n",
    "    #xt = tfidf.fit_transform(final_corpus)\n",
    "    xt_bi = bigram_tfidf.fit_transform(data_cleaned[\"text\"])\n",
    "    y = df['sentiment'].values\n",
    "\n",
    "    # Inicializar StratifiedKFold con el número deseado de pliegues (folds)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterar sobre los pliegues generados por StratifiedKFold\n",
    "    for train_index, test_index in kfold.split(xt_bi, y):\n",
    "        X_train, X_test = xt[train_index], xt[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    scores = []\n",
    "    for model in models:\n",
    "        # Do it for the default dataset\n",
    "        cv_scores = cross_val_score(model, xt_bi, y, cv=5)\n",
    "\n",
    "        # Almacenar las puntuaciones\n",
    "        scores.append(cv_scores.mean())\n",
    "\n",
    "        # Agregar las puntuaciones al diccionario\n",
    "    results[\"default\"] = scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "def apply_data_augmentation(corpus, tecnica):\n",
    "    if tecnica == \"Synonyms\":\n",
    "        synonyms_corpus = clean_corpus(corpus)\n",
    "        synonyms_corpus = augment_with_synonyms(synonyms_corpus)\n",
    "        #xt = tfidf.fit_transform(synonyms_corpus)\n",
    "        xt_bi = bigram_tfidf.fit_transform(synonyms_corpus)\n",
    "        \n",
    "    elif tecnica == \"Shuffle\":\n",
    "        shuffle_corpus = clean_corpus(corpus)\n",
    "        shuffle_corpus = augment_with_shuffle_words(shuffle_corpus)\n",
    "        #xt = tfidf.fit_transform(shuffle_corpus)\n",
    "        xt_bi = bigram_tfidf.fit_transform(shuffle_corpus)\n",
    "        \n",
    "    elif tecnica == \"1Syn2Shuff\":\n",
    "        synonyms_corpus = clean_corpus(corpus)\n",
    "        synonyms_corpus = augment_with_synonyms(synonyms_corpus)\n",
    "        shuffle_corpus = augment_with_shuffle_words(synonyms_corpus)\n",
    "        #xt = tfidf.fit_transform(shuffle_corpus)\n",
    "        xt_bi = bigram_tfidf.fit_transform(shuffle_corpus)\n",
    "        \n",
    "    elif tecnica == \"2Syn1Shuff\":\n",
    "        shuffle_corpus = clean_corpus(corpus)\n",
    "        shuffle_corpus = augment_with_shuffle_words(shuffle_corpus)\n",
    "        synonyms_corpus = augment_with_synonyms(shuffle_corpus)\n",
    "        #xt = tfidf.fit_transform(synonyms_corpus)\n",
    "        xt_bi = bigram_tfidf.fit_transform(synonyms_corpus)\n",
    "        \n",
    "    y = df['sentiment'].values\n",
    "    \n",
    "    # Inicializar StratifiedKFold con el número deseado de pliegues (folds)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterar sobre los pliegues generados por StratifiedKFold\n",
    "    for train_index, test_index in kfold.split(xt_bi, y):\n",
    "        X_train, X_test = xt[train_index], xt[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "    \n",
    "# Definir las técnicas de data augmentation\n",
    "tecnicas = ['Synonyms', 'Shuffle', '1Syn2Shuff', '2Syn1Shuff']\n",
    "\n",
    "# Crear un diccionario con la puntuación default para almacenar las puntuaciones de los modelos después de aplicar data augmentation\n",
    "results = apply_default(final_corpus, models)\n",
    "\n",
    "# Here we start iterating and evluating models\n",
    "for tecnica in tecnicas:\n",
    "    scores = []\n",
    "    for model in models:\n",
    "        # Aplicar la técnica de data augmentation\n",
    "        X_train_augmented, y_train_augmented = apply_data_augmentation(corpus, tecnica)\n",
    "        \n",
    "        # Realizar la evaluación del modelo con validación cruzada\n",
    "        cv_scores = cross_val_score(model, X_train_augmented, y_train_augmented, cv=5)\n",
    "        \n",
    "        # Almacenar las puntuaciones\n",
    "        scores.append(cv_scores.mean())\n",
    "    \n",
    "    # Agregar las puntuaciones al diccionario\n",
    "    results[tecnica] = scores\n",
    "    \n",
    "# Crear un DataFrame a partir del diccionario\n",
    "data_augmentation_results = pd.DataFrame(results)\n",
    "\n",
    "# Diccionario de mapeo de índices a nombres de modelos\n",
    "model_names = {0: 'LogisticRegression', 1: 'DecisionTree', 2: 'LinearSVC', 3: 'NB', 4: 'RandomForest'}\n",
    "\n",
    "# Cambiar los índices por nombres de modelos\n",
    "data_augmentation_results.rename(index=model_names, inplace=True)\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "#data_augmentation_results.to_csv('data_augmentation_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b54d3",
   "metadata": {},
   "source": [
    "After using multiple data augmentation techniques (also adding the default), I store the results and check wich one is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d80ed0",
   "metadata": {},
   "source": [
    "#### Data augmentation scores results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949f84a",
   "metadata": {},
   "source": [
    "Just to be sure that I choose the best data set I will plot the same graphs as done before and see the diffrences with the different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71d001",
   "metadata": {},
   "source": [
    "## Auxiliar cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991adea",
   "metadata": {},
   "source": [
    "This is just an auxiliar cell that will create the x_train and y_train sets to evaluate the models with different data (trying with different combinations of data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40cf32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "\n",
    "# Syn\n",
    "#syn_corpus = augment_with_synonyms(corpus)\n",
    "\n",
    "# Shuffle\n",
    "shuffle_corpus = augment_with_shuffle_words(corpus)\n",
    "\n",
    "# SynShuffle\n",
    "#syn_corpus = augment_with_synonyms(corpus)\n",
    "#shuffle_corpus = augment_with_shuffle_words(syn_corpus)\n",
    "\n",
    "# ShuffSyn\n",
    "#shuffle_corpus = augment_with_shuffle_words(corpus)\n",
    "#syn_corpus = augment_with_synonyms(shuffle_corpus)\n",
    "\n",
    "xt = bigram_tfidf.fit_transform(shuffle_corpus)\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Inicializar StratifiedKFold con el número deseado de pliegues (folds)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterar sobre los pliegues generados por StratifiedKFold\n",
    "for train_index, test_index in kfold.split(xt, y):\n",
    "    X_train, X_test = xt[train_index], xt[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4059c",
   "metadata": {},
   "source": [
    "# Comparing models learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79c152",
   "metadata": {},
   "source": [
    "After applying these techniques we still see that the model 1,3 and 4 the lineal ones that have algorithms that kind of solve the unbalance problem have a good score but the model 2 and 5, the randomForest and the decisionTree ones, altought they have improved their scores and the imabalance is reduced, the std is still high and still have the imbalance problem.\\\n",
    "In order to solve that I will try to apply another imbalance thecniques to balance the data before evaluating the models and then see if the performance improves or not. This will be done with the models that have the problem, the decissionTree and RandomForest but for that, as we said before we will use the augmented dataset that best fits our model (shuffle_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f67c1",
   "metadata": {},
   "source": [
    "Here we will use the shuffleled augmented dataset because in the pdf we have seen that it makes the most imbalanced models solve the imbalanced problems better than the other augmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = LinearSVC()\n",
    "model4 = MultinomialNB()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "\n",
    "results = {}\n",
    "\n",
    "techniques = [RandomOverSampler(), RandomUnderSampler(), SMOTEENN()]\n",
    "\n",
    "for tech in techniques:\n",
    "    scores = []\n",
    "    for model in models:\n",
    "        shuffle_corpus = augment_with_shuffle_words(final_corpus)\n",
    "\n",
    "        xt = tfidf.fit_transform(shuffle_corpus)\n",
    "        y = df['sentiment'].values\n",
    "\n",
    "        # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Iterar sobre los pliegues generados por StratifiedKFold\n",
    "        for train_index, test_index in kfold.split(xt, y):\n",
    "            X_train, X_test = xt[train_index], xt[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "        # Realizar el muestreo estratificado\n",
    "        sampler = tech\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Entrenar un modelo de Random Forest con el conjunto de datos equilibrado\n",
    "        model = model\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5)\n",
    "        \n",
    "        # Almacenar las puntuaciones\n",
    "        scores.append(cv_scores.mean())\n",
    "    \n",
    "    # Agregar las puntuaciones al diccionario\n",
    "    results[tech] = scores\n",
    "    \n",
    "# Crear un DataFrame a partir del diccionario\n",
    "data_augmentation_results_sampling = pd.DataFrame(results)\n",
    "\n",
    "# Diccionario de mapeo de índices a nombres de modelos\n",
    "model_names = {0: 'LogisticRegression', 1: 'DecisionTree', 2: 'LinearSVC', 3: 'NB', 4: 'RandomForest'}\n",
    "\n",
    "# Cambiar los índices por nombres de modelos\n",
    "data_augmentation_results_sampling.rename(index=model_names, inplace=True)\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "#data_augmentation_results.to_csv('data_augmentation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_results_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ce317",
   "metadata": {},
   "source": [
    "## Future work possible implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e42c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(tfidf_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(tfidf_train.toarray(), y_train, validation_data=(tfidf_val.toarray(), y_val), epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 250.472894,
   "end_time": "2023-02-06T12:18:41.002804",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-06T12:14:30.529910",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
